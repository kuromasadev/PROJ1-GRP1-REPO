{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping HTML Site for Music Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the archival site into a request and into the web scraper\n",
    "url = \"https://kworb.net/ww/archive/\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Beautiful Soup to find all the anchor tags within the \"pre\" tag of the HTML content\n",
    "pre_tag = soup.find('pre')\n",
    "a_tags = pre_tag.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the list of links and use requests to make a request to each link but only that start between 2015-2021\n",
    "links_2015 = []\n",
    "links_2016 = []\n",
    "links_2017 = []\n",
    "links_2018 = []\n",
    "links_2019 = []\n",
    "links_2020 = []\n",
    "links_2021 = []\n",
    "\n",
    "for a in a_tags:\n",
    "    if a.has_attr('href') and a['href'].startswith(('2015')):\n",
    "        links_2015.append(a['href'])\n",
    "    if a.has_attr('href') and a['href'].startswith(('2016')):\n",
    "       links_2016.append(a['href'])\n",
    "    if a.has_attr('href') and a['href'].startswith(('2017')):\n",
    "       links_2017.append(a['href'])\n",
    "    if a.has_attr('href') and a['href'].startswith(('2018')):\n",
    "       links_2018.append(a['href'])\n",
    "    if a.has_attr('href') and a['href'].startswith(('2019')):\n",
    "       links_2019.append(a['href'])\n",
    "    if a.has_attr('href') and a['href'].startswith(('2020')):\n",
    "       links_2020.append(a['href'])\n",
    "    if a.has_attr('href') and a['href'].startswith(('2021')):\n",
    "       links_2021.append(a['href'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing HTML Anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20150101.html', '20150102.html', '20150103.html', '20150104.html', '20150105.html', '20150106.html', '20150107.html', '20150108.html', '20150109.html', '20150110.html', '20150111.html', '20150112.html', '20150113.html', '20150114.html', '20150115.html', '20150116.html', '20150117.html', '20150118.html', '20150119.html', '20150120.html', '20150121.html', '20150122.html', '20150123.html', '20150124.html', '20150125.html', '20150126.html', '20150127.html', '20150128.html', '20150129.html', '20150130.html', '20150131.html', '20150201.html', '20150202.html', '20150203.html', '20150204.html', '20150205.html', '20150206.html', '20150207.html', '20150208.html', '20150209.html', '20150210.html', '20150211.html', '20150212.html', '20150213.html', '20150214.html', '20150215.html', '20150216.html', '20150217.html', '20150218.html', '20150219.html', '20150220.html', '20150221.html', '20150222.html', '20150223.html', '20150224.html', '20150225.html', '20150226.html', '20150227.html', '20150228.html', '20150301.html', '20150302.html', '20150303.html', '20150304.html', '20150305.html', '20150306.html', '20150307.html', '20150308.html', '20150309.html', '20150310.html', '20150311.html', '20150312.html', '20150313.html', '20150314.html', '20150315.html', '20150316.html', '20150317.html', '20150318.html', '20150319.html', '20150320.html', '20150321.html', '20150322.html', '20150323.html', '20150324.html', '20150325.html', '20150326.html', '20150327.html', '20150328.html', '20150329.html', '20150330.html', '20150331.html', '20150401.html', '20150402.html', '20150403.html', '20150404.html', '20150405.html', '20150406.html', '20150407.html', '20150408.html', '20150409.html', '20150410.html', '20150411.html', '20150412.html', '20150413.html', '20150414.html', '20150415.html', '20150416.html', '20150417.html', '20150418.html', '20150419.html', '20150420.html', '20150421.html', '20150422.html', '20150423.html', '20150424.html', '20150425.html', '20150426.html', '20150427.html', '20150428.html', '20150429.html', '20150430.html', '20150501.html', '20150502.html', '20150503.html', '20150504.html', '20150505.html', '20150506.html', '20150507.html', '20150508.html', '20150509.html', '20150510.html', '20150511.html', '20150512.html', '20150513.html', '20150514.html', '20150515.html', '20150516.html', '20150517.html', '20150518.html', '20150519.html', '20150520.html', '20150521.html', '20150522.html', '20150523.html', '20150524.html', '20150525.html', '20150526.html', '20150527.html', '20150528.html', '20150529.html', '20150530.html', '20150531.html', '20150601.html', '20150602.html', '20150603.html', '20150604.html', '20150605.html', '20150606.html', '20150607.html', '20150608.html', '20150609.html', '20150610.html', '20150611.html', '20150612.html', '20150613.html', '20150614.html', '20150615.html', '20150616.html', '20150617.html', '20150618.html', '20150619.html', '20150620.html', '20150621.html', '20150622.html', '20150623.html', '20150624.html', '20150625.html', '20150626.html', '20150627.html', '20150628.html', '20150629.html', '20150630.html', '20150701.html', '20150702.html', '20150703.html', '20150704.html', '20150705.html', '20150706.html', '20150707.html', '20150708.html', '20150709.html', '20150710.html', '20150711.html', '20150712.html', '20150713.html', '20150714.html', '20150715.html', '20150716.html', '20150717.html', '20150718.html', '20150719.html', '20150720.html', '20150721.html', '20150722.html', '20150723.html', '20150724.html', '20150725.html', '20150726.html', '20150727.html', '20150728.html', '20150729.html', '20150730.html', '20150731.html', '20150801.html', '20150802.html', '20150803.html', '20150804.html', '20150805.html', '20150806.html', '20150807.html', '20150808.html', '20150809.html', '20150810.html', '20150811.html', '20150812.html', '20150813.html', '20150814.html', '20150815.html', '20150816.html', '20150817.html', '20150818.html', '20150819.html', '20150820.html', '20150821.html', '20150822.html', '20150823.html', '20150824.html', '20150825.html', '20150826.html', '20150827.html', '20150828.html', '20150829.html', '20150830.html', '20150831.html', '20150901.html', '20150902.html', '20150903.html', '20150904.html', '20150905.html', '20150906.html', '20150907.html', '20150908.html', '20150909.html', '20150910.html', '20150911.html', '20150912.html', '20150913.html', '20150914.html', '20150915.html', '20150916.html', '20150917.html', '20150918.html', '20150919.html', '20150920.html', '20150921.html', '20150922.html', '20150923.html', '20150924.html', '20150925.html', '20150926.html', '20150927.html', '20150928.html', '20150929.html', '20150930.html', '20151001.html', '20151002.html', '20151003.html', '20151004.html', '20151005.html', '20151006.html', '20151007.html', '20151008.html', '20151009.html', '20151010.html', '20151011.html', '20151012.html', '20151013.html', '20151014.html', '20151015.html', '20151016.html', '20151017.html', '20151018.html', '20151019.html', '20151020.html', '20151021.html', '20151022.html', '20151023.html', '20151024.html', '20151025.html', '20151026.html', '20151027.html', '20151028.html', '20151029.html', '20151030.html', '20151031.html', '20151101.html', '20151102.html', '20151103.html', '20151104.html', '20151105.html', '20151106.html', '20151107.html', '20151108.html', '20151109.html', '20151110.html', '20151111.html', '20151112.html', '20151113.html', '20151114.html', '20151115.html', '20151116.html', '20151117.html', '20151118.html', '20151119.html', '20151120.html', '20151121.html', '20151122.html', '20151123.html', '20151124.html', '20151125.html', '20151126.html', '20151127.html', '20151128.html', '20151129.html', '20151130.html', '20151201.html', '20151202.html', '20151203.html', '20151204.html', '20151205.html', '20151206.html', '20151207.html', '20151208.html', '20151209.html', '20151210.html', '20151211.html', '20151212.html', '20151213.html', '20151214.html', '20151215.html', '20151216.html', '20151217.html', '20151218.html', '20151219.html', '20151220.html', '20151221.html', '20151222.html', '20151223.html', '20151224.html', '20151225.html', '20151226.html', '20151227.html', '20151228.html', '20151229.html', '20151230.html', '20151231.html']\n"
     ]
    }
   ],
   "source": [
    "print(links_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150101.html\n"
     ]
    }
   ],
   "source": [
    "# Verifying the url composition\n",
    "url_test = str(links_2015[0])\n",
    "print(url_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Use Beautiful Soup to parse the HTML content of each link and find the table tag within the HTML content.\n",
    "link_test = requests.get(url+url_test)\n",
    "link_soup_test = BeautifulSoup(link_test.content, 'html.parser')\n",
    "\n",
    "# Step 2: Find the table in the html code\n",
    "table_test = link_soup_test.find('table')\n",
    "\n",
    "# Step 3: Use pandas library to read the html table into dataframe\n",
    "df_test = pd.read_html(str(table_test))[0].iloc[:,:9]\n",
    "\n",
    "# Step 4: Extract the date code from the url link as use it as a week label for future sorting\n",
    "weeklabel = url_test[:8]\n",
    "df_test['Week'] = weeklabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>Pos+</th>\n",
       "      <th>Artist and Title</th>\n",
       "      <th>Days</th>\n",
       "      <th>Peak</th>\n",
       "      <th>Pts</th>\n",
       "      <th>Pts+</th>\n",
       "      <th>TPts</th>\n",
       "      <th>US</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>=</td>\n",
       "      <td>Taylor Swift - Blank Space</td>\n",
       "      <td>67</td>\n",
       "      <td>1(29)</td>\n",
       "      <td>20632</td>\n",
       "      <td>-115</td>\n",
       "      <td>1.113</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20150101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>=</td>\n",
       "      <td>Mark Ronson - Uptown Funk</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>19255</td>\n",
       "      <td>49</td>\n",
       "      <td>0.729</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20150101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>+1</td>\n",
       "      <td>Ed Sheeran - Thinking Out Loud</td>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>18928</td>\n",
       "      <td>464</td>\n",
       "      <td>1.444</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20150101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>Taylor Swift - Shake It Off</td>\n",
       "      <td>136</td>\n",
       "      <td>1(47)</td>\n",
       "      <td>18626</td>\n",
       "      <td>34</td>\n",
       "      <td>2.606</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20150101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>=</td>\n",
       "      <td>Meghan Trainor - All About That Bass</td>\n",
       "      <td>164</td>\n",
       "      <td>1(22)</td>\n",
       "      <td>16812</td>\n",
       "      <td>70</td>\n",
       "      <td>2.539</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20150101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pos Pos+                      Artist and Title Days   Peak    Pts  Pts+  \\\n",
       "0    1    =            Taylor Swift - Blank Space   67  1(29)  20632  -115   \n",
       "1    2    =             Mark Ronson - Uptown Funk   52      2  19255    49   \n",
       "2    3   +1        Ed Sheeran - Thinking Out Loud  120      2  18928   464   \n",
       "3    4   -1           Taylor Swift - Shake It Off  136  1(47)  18626    34   \n",
       "4    5    =  Meghan Trainor - All About That Bass  164  1(22)  16812    70   \n",
       "\n",
       "    TPts   US      Week  \n",
       "0  1.113  2.0  20150101  \n",
       "1  0.729  1.0  20150101  \n",
       "2  1.444  6.0  20150101  \n",
       "3  2.606  5.0  20150101  \n",
       "4  2.539  7.0  20150101  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lists of links\n",
    "links = [links_2015, links_2016, links_2017, links_2018, links_2019, links_2020, links_2021]\n",
    "week_allcharts_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Year progress: 100%|██████████| 7/7 [1:13:38<00:00, 631.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each list of links\n",
    "for link_list in tqdm(links, desc='Overall Year progress', position=0):\n",
    "    # Loop through each link in the list\n",
    "    for link in tqdm(link_list, desc=f'Links in {link_list[0][:4]}', position=1, leave=False):\n",
    "        try:\n",
    "        # Step 1: Use Beautiful Soup to parse the HTML content of each link and find the table tag within the HTML content.\n",
    "            link_final = requests.get(url+link)\n",
    "            link_soup_final = BeautifulSoup(link_final.content, 'html.parser')\n",
    "\n",
    "        # Step 2: Find the table in the html code\n",
    "            table_final = link_soup_final.find('table')\n",
    "\n",
    "        # Step 3: Use pandas library to read the html table into dataframe\n",
    "            weekchart_df = pd.read_html(str(table_final))[0].iloc[:,:9]\n",
    "\n",
    "        # Step 4: Extract the date code from the url link as use it as a week label for future sorting\n",
    "            weeklabel = link[:8]\n",
    "            weekchart_df ['Week'] = weeklabel\n",
    "\n",
    "        # Step 5: Append the dataframe to the list of dataframes\n",
    "            week_allcharts_df.append(weekchart_df)\n",
    "        except:\n",
    "            print(f'Error processing link: {link}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pos</th>\n",
       "      <th>P+</th>\n",
       "      <th>Artist and Title</th>\n",
       "      <th>Days</th>\n",
       "      <th>Pk</th>\n",
       "      <th>(x?)</th>\n",
       "      <th>Pts</th>\n",
       "      <th>Pts+</th>\n",
       "      <th>TPts</th>\n",
       "      <th>Week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>+1</td>\n",
       "      <td>Elton John &amp; Dua Lipa - Cold Heart</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>(x19)</td>\n",
       "      <td>18842</td>\n",
       "      <td>890</td>\n",
       "      <td>2.522</td>\n",
       "      <td>20211231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>Adele - Easy On Me</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>(x49)</td>\n",
       "      <td>18726</td>\n",
       "      <td>332</td>\n",
       "      <td>1.561</td>\n",
       "      <td>20211231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>+13</td>\n",
       "      <td>Jaymes Young - Happiest Year</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>(x1)</td>\n",
       "      <td>15922</td>\n",
       "      <td>8216</td>\n",
       "      <td>0.094</td>\n",
       "      <td>20211231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>=</td>\n",
       "      <td>Acraze - Do It To It</td>\n",
       "      <td>78</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14353</td>\n",
       "      <td>822</td>\n",
       "      <td>0.783</td>\n",
       "      <td>20211231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-2</td>\n",
       "      <td>Alesso &amp; Katy Perry - When I'm Gone</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>(x1)</td>\n",
       "      <td>13608</td>\n",
       "      <td>-1009</td>\n",
       "      <td>0.040</td>\n",
       "      <td>20211231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pos   P+                     Artist and Title  Days  Pk   (x?)    Pts  \\\n",
       "0    1   +1   Elton John & Dua Lipa - Cold Heart   141   1  (x19)  18842   \n",
       "1    2   -1                   Adele - Easy On Me    78   1  (x49)  18726   \n",
       "2    3  +13         Jaymes Young - Happiest Year    14   3   (x1)  15922   \n",
       "3    4    =                 Acraze - Do It To It    78   4    NaN  14353   \n",
       "4    5   -2  Alesso & Katy Perry - When I'm Gone     3   3   (x1)  13608   \n",
       "\n",
       "   Pts+   TPts      Week  \n",
       "0   890  2.522  20211231  \n",
       "1   332  1.561  20211231  \n",
       "2  8216  0.094  20211231  \n",
       "3   822  0.783  20211231  \n",
       "4 -1009  0.040  20211231  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekchart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all dataframes in week_allcharts_df list into a single dataframe\n",
    "week_allcharts_df = pd.concat(week_allcharts_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 565600 entries, 0 to 565599\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Pos               565600 non-null  int64  \n",
      " 1   Pos+              133000 non-null  object \n",
      " 2   Artist and Title  565600 non-null  object \n",
      " 3   Days              565600 non-null  object \n",
      " 4   Peak              133000 non-null  object \n",
      " 5   Pts               565600 non-null  int64  \n",
      " 6   Pts+              565600 non-null  int64  \n",
      " 7   TPts              565600 non-null  float64\n",
      " 8   US                65273 non-null   float64\n",
      " 9   Week              565600 non-null  object \n",
      " 10  P+                432600 non-null  object \n",
      " 11  Pk                432600 non-null  float64\n",
      " 12  (x?)              91685 non-null   object \n",
      "dtypes: float64(3), int64(3), object(7)\n",
      "memory usage: 56.1+ MB\n"
     ]
    }
   ],
   "source": [
    "week_allcharts_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/565600 [00:56<?, ?it/s]\n",
      "100%|██████████| 565600/565600 [00:04<00:00, 122786.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# export the dataframe to a CSV file with high efficiency\n",
    "chunk_size = 10000 # adjust this to a size that works for your system\n",
    "df_list = [chunk for _, chunk in week_allcharts_df.groupby(np.arange(len(week_allcharts_df)) // chunk_size)]\n",
    "\n",
    "with open('../test chamber/week_allcharts_df_v01.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    pbar = tqdm(total=len(week_allcharts_df))\n",
    "    for chunk in df_list:\n",
    "        chunk.to_csv(f, index=False, header=False)\n",
    "        pbar.update(len(chunk))\n",
    "    pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev00",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
